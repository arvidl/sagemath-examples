{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dedd2a6",
   "metadata": {},
   "source": [
    "# 07-ai-in-sage.ipynb\n",
    "\n",
    "\n",
    "A.L. 2023-08-21 (Ubuntu 2022.04, Sage 10.1)<br>\n",
    "A.L. 2024-07-22 (MacBook Air 11 inch, Mid 2012, MacOs 10.15.7, Sage 10.3, using `brew install llm`)\n",
    "\n",
    "\n",
    "- https://github.com/jupyterlab/jupyter-ai\n",
    "\n",
    "- https://openai.com/api/\n",
    "\n",
    "You can find your API key at https://platform.openai.com/account/api-keys.\n",
    "\n",
    "\n",
    "```\n",
    "echo \"export OPENAI_API_KEY='yourkey'\" >> ~/.zshrc   (or  ~/.bashrc )\n",
    "source ~/.zshrc\n",
    "echo $OPENAI_API_KEY\n",
    "```\n",
    "or\n",
    "```\n",
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "\n",
    "\n",
    "See also: https://sagemanifolds.obspm.fr/install_ubuntu.html\n",
    "\n",
    "Check sage installation and start a Jupyter notebook with the SageMath 9.6 kernel:\n",
    "\n",
    "```\n",
    "sage -c 'print(version())\n",
    "sage -notebook\n",
    "```\n",
    "\n",
    "\n",
    "Installing R-packages:\n",
    "```\n",
    "sage -R\n",
    "\n",
    "> install.packages(\"lme4\")\n",
    "```\n",
    "\n",
    "Installing Python packages (in the sage kernel):\n",
    "```\n",
    "> sage --pip install pandas\n",
    "> sage --pip install seaborn\n",
    "> sage --pip install networkx\n",
    "> sage --pip install igraph\n",
    "> # sage --pip install pycairo\n",
    "> # sage --pip install cairocffi\n",
    "> # sage --pip install leidenalg\n",
    "> sage --pip install jupyterlab\n",
    "> sage --pip install altair vega_datasets      # https://altair-viz.github.io/index.html\n",
    "> sage --pip install pygraphviz\n",
    "\n",
    "> sage --pip install openai\n",
    "> sage --pip install jupyter_ai\n",
    "> sage --pip install python-dotenv\n",
    "```\n",
    "\n",
    "```\n",
    "%reload_ext jupyter_ai\n",
    "\n",
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c30bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211036a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "  version   Prints Jupyter-AI version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f490af14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li></ul> |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x162b12d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201a507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is no model provider with ID `openai`."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x162b122d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY=os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9e5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "OPENAI_API_KEY=$API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a18544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x162b2b1d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate the 2D heat equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958e0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x162b29010>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55989eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCM of 3 and 5 is 15\n",
      "LCM of 9 and 12 is 36\n",
      "LCM of 8 and 14 is 56\n",
      "LCM of 15 and 20 is 60\n",
      "LCM of 24 and 36 is 72\n",
      "LCM of 9 and 28 is 252\n"
     ]
    }
   ],
   "source": [
    "def lcm(a, b):\n",
    "    # Find the maximum of a and b\n",
    "    if a > b:\n",
    "        max_num = a\n",
    "    else:\n",
    "        max_num = b\n",
    "\n",
    "    # Iterate and find the lowest common multiple\n",
    "    while True:\n",
    "        if max_num % a == 0 and max_num % b == 0:\n",
    "            return max_num\n",
    "        max_num += 1\n",
    "\n",
    "def run_lcm_tests():\n",
    "    # Run test cases\n",
    "    test_cases = [(3, 5), (9, 12), (8, 14), (15, 20), (24, 36), (9,28)]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        result = lcm(a, b)\n",
    "        print(f\"LCM of {a} and {b} is {result}\")\n",
    "\n",
    "# Run the test cases\n",
    "run_lcm_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d495c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = \"Herman Wildenvvey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52b6dee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x168f0e4d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Write a poem in the style of {poet} in Norwegian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247ddeb",
   "metadata": {},
   "source": [
    "## llm   Simon Willison\n",
    "\n",
    "https://github.com/simonw/llm\n",
    "\n",
    "https://llm.datasette.io/en/stable/usage.html\n",
    "\n",
    "```bash\n",
    "If you have an OpenAI API key you can get started using the OpenAI models right away.\n",
    "\n",
    "As an alternative to OpenAI, you can install plugins to access models by other providers, including models that can be installed and run on your own device.\n",
    "\n",
    "Save your OpenAI API key like this:\n",
    "\n",
    "llm keys set openai\n",
    "This will prompt you for your key like so:\n",
    "\n",
    "Enter key: <paste here>\n",
    "```\n",
    "\n",
    "Now that you've saved a key you can run a prompt like this:\n",
    "```bash\n",
    "llm \"Five cute names for a pet penguin\"\n",
    "```\n",
    "\n",
    "The OpenAI API key is saved in keys.json:\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % ls -l /Users/arvid/Library/Application\\ Support/io.datasette.llm\n",
    "total 136\n",
    "-rw-------  1 arvid  staff    146 Jul 20 17:47 keys.json\n",
    "-rw-r--r--  1 arvid  staff  65536 Jul 20 20:09 logs.db\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm models\n",
    "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
    "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
    "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
    "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
    "OpenAI Chat: gpt-4-1106-preview\n",
    "OpenAI Chat: gpt-4-0125-preview\n",
    "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
    "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
    "OpenAI Chat: gpt-4o (aliases: 4o)\n",
    "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
    "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
    "```\n",
    "\n",
    "To describe how the code a file works, try this:\n",
    "```bash\n",
    "cat mycode.py | llm -s \"Explain this code\"\n",
    "```\n",
    "\n",
    "Additional examples and functions:\n",
    "\n",
    "```bash\n",
    "llm 'Ten names for cheesecakes' -m gpt-4o\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm models --options\n",
    "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
    "  temperature: float\n",
    "    What sampling temperature to use, between 0 and 2. Higher values like\n",
    "    0.8 will make the output more random, while lower values like 0.2 will\n",
    "    make it more focused and deterministic.\n",
    "  max_tokens: int\n",
    "    Maximum number of tokens to generate.\n",
    "  top_p: float\n",
    "    An alternative to sampling with temperature, called nucleus sampling,\n",
    "    where the model considers the results of the tokens with top_p\n",
    "    probability mass. So 0.1 means only the tokens comprising the top 10%\n",
    "    probability mass are considered. Recommended to use top_p or\n",
    "    temperature but not both.\n",
    "  frequency_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on their existing frequency in the text so far, decreasing the model's\n",
    "    likelihood to repeat the same line verbatim.\n",
    "  presence_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on whether they appear in the text so far, increasing the model's\n",
    "    likelihood to talk about new topics.\n",
    "  stop: str\n",
    "    A string where the API will stop generating further tokens.\n",
    "  logit_bias: dict, str\n",
    "    Modify the likelihood of specified tokens appearing in the completion.\n",
    "    Pass a JSON string like '{\"1712\":-100, \"892\":-100, \"1489\":-100}'\n",
    "  seed: int\n",
    "    Integer seed to attempt to sample deterministically\n",
    "  json_object: boolean\n",
    "    Output a valid JSON object {...}. Prompt must mention JSON.\n",
    "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-1106-preview\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-0125-preview\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4o (aliases: 4o)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
    "  temperature: float\n",
    "    What sampling temperature to use, between 0 and 2. Higher values like\n",
    "    0.8 will make the output more random, while lower values like 0.2 will\n",
    "    make it more focused and deterministic.\n",
    "  max_tokens: int\n",
    "    Maximum number of tokens to generate.\n",
    "  top_p: float\n",
    "    An alternative to sampling with temperature, called nucleus sampling,\n",
    "    where the model considers the results of the tokens with top_p\n",
    "    probability mass. So 0.1 means only the tokens comprising the top 10%\n",
    "    probability mass are considered. Recommended to use top_p or\n",
    "    temperature but not both.\n",
    "  frequency_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on their existing frequency in the text so far, decreasing the model's\n",
    "    likelihood to repeat the same line verbatim.\n",
    "  presence_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on whether they appear in the text so far, increasing the model's\n",
    "    likelihood to talk about new topics.\n",
    "  stop: str\n",
    "    A string where the API will stop generating further tokens.\n",
    "  logit_bias: dict, str\n",
    "    Modify the likelihood of specified tokens appearing in the completion.\n",
    "    Pass a JSON string like '{\"1712\":-100, \"892\":-100, \"1489\":-100}'\n",
    "  seed: int\n",
    "    Integer seed to attempt to sample deterministically\n",
    "  logprobs: int\n",
    "    Include the log probabilities of most likely N per token\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```\n",
    "\n",
    "OpenAI embedding models\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm embed-models\n",
    "ada-002 (aliases: ada)\n",
    "3-small\n",
    "3-large\n",
    "3-small-512\n",
    "3-large-256\n",
    "3-large-1024\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b41ae6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_llm(prompt):\n",
    "    binary_path = \"/usr/local/bin/llm\"\n",
    "    command = [binary_path, prompt]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "        \n",
    "        print(\"LLM Response:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"Standard Error:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        # print(f\"Return Code: {result.returncode}\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(f\"Standard Output: {e.stdout}\")\n",
    "        print(f\"Standard Error: {e.stderr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd9fa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "Certainly! To compute the lowest common multiple (LCM) of two integers, we can use the relationship between the greatest common divisor (GCD) and LCM, which is given by the formula:\n",
      "\n",
      "\\[\n",
      "\\text{LCM}(a, b) = \\frac{|a \\cdot b|}{\\text{GCD}(a, b)}\n",
      "\\]\n",
      "\n",
      "We can implement this function in Python using the built-in `math.gcd` function for calculating the GCD. Here’s an implementation along with a function that runs 5 test cases.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def lcm(a, b):\n",
      "    \"\"\"Compute the Lowest Common Multiple of two integers a and b.\"\"\"\n",
      "    return abs(a * b) // math.gcd(a, b)\n",
      "\n",
      "def test_lcm():\n",
      "    \"\"\"Run 5 test cases for the LCM function.\"\"\"\n",
      "    test_cases = [\n",
      "        (12, 15),  # LCM should be 60\n",
      "        (4, 5),    # LCM should be 20\n",
      "        (21, 6),   # LCM should be 42\n",
      "        (0, 5),    # LCM should be 0 since one number is 0\n",
      "        (-4, -6)   # LCM should be 12 for negative numbers\n",
      "    ]\n",
      "    \n",
      "    for a, b in test_cases:\n",
      "        result = lcm(a, b)\n",
      "        print(f\"LCM({a}, {b}) = {result}\")\n",
      "\n",
      "# Run the tests\n",
      "test_lcm()\n",
      "```\n",
      "\n",
      "### Explanation of the Code:\n",
      "1. **LCM Function**:\n",
      "   - The `lcm` function takes two integers `a` and `b`.\n",
      "   - It calculates the GCD using `math.gcd(a, b)` and then computes LCM using the formula mentioned earlier.\n",
      "   \n",
      "2. **Testing Function**:\n",
      "   - The `test_lcm` function defines a list of test cases, each a tuple representing a pair of integers.\n",
      "   - It iterates through each case, calculating the LCM and printing the result.\n",
      "   \n",
      "3. **Running Tests**:\n",
      "   - Finally, `test_lcm()` is called to execute the tests.\n",
      "\n",
      "Feel free to run this code in your Python environment or modify it to include additional test cases!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\"\n",
    "run_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a3cde65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcm(15, 20) = 60\n",
      "lcm(9, 28) = 252\n",
      "lcm(5, 0) = 0\n",
      "lcm(0, 10) = 0\n",
      "lcm(12, 15) = 60\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def lcm(a, b):\n",
    "    \"\"\"Calculate the least common multiple of two integers a and b.\"\"\"\n",
    "    if a == 0 or b == 0:\n",
    "        return 0  # LCM of zero with any number is defined as zero\n",
    "    \n",
    "    return abs(a * b) // math.gcd(a, b)\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"Run a set of test cases for the lcm function.\"\"\"\n",
    "    test_cases = [\n",
    "        (15, 20),  # Expected LCM: 60\n",
    "        (9, 28),   # Expected LCM: 252\n",
    "        (5, 0),    # Expected LCM: 0 (with zero)\n",
    "        (0, 10),   # Expected LCM: 0 (with zero)\n",
    "        (12, 15)   # Expected LCM: 60\n",
    "    ]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        print(f\"lcm({a}, {b}) = {lcm(a, b)}\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6ed2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "Selv om jeg ikke helt kan fange stilen til Herman Wildenvey perfekt, her er et forsøk på å skrive et dikt inspirert av hans språk og tematikker:\n",
      "\n",
      "**Morgengryets Sang**\n",
      "\n",
      "Når daggryet kysser natten farvel,  \n",
      "og stjernene svinner i lysets spill,  \n",
      "da hvisker vinden i bjørketrærnes elv,  \n",
      "om drømmer som vokser, om sjelens mil.\n",
      "\n",
      "Den dype skogen med sine skjulte rom,  \n",
      "der fuglesang svinner, men hjertet forstår,  \n",
      "har minner av lykke, av kjærlighetstrom,  \n",
      "der skygger og lys i en dans alltid går.\n",
      "\n",
      "Se! Sjøens speil under skyers blå klær,  \n",
      "reflekterer lengsler, en evig begjær.  \n",
      "For livet er skjørt som et rosenbladstøv,  \n",
      "som faller i stillhet, men bærer et løft.\n",
      "\n",
      "Å vandre i tankene, fritt som en vind,  \n",
      "å søke det skjulte, det rare og mild,  \n",
      "for hver eneste stund er et skattkammer sinn,  \n",
      "med poesi gjemt i det enkle og vild.\n",
      "\n",
      "Så hør nå, min sjel, i denne stunden,  \n",
      "når natten gir plass til den lysende gryn,  \n",
      "la hjertet få tale, i sansenes runden,  \n",
      "og leve i nuet, med natur og med dyne.\n",
      "\n",
      "Livet er mer enn bare et spor,  \n",
      "det er musikk som i hjertet får rom.  \n",
      "Fra mørket til lyset, vi stiger og får,  \n",
      "en vei fylt av håp, av drømmer i ånd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poet = \"Herman Wildenvvey\"\n",
    "prompt = f\"Write a poem in the style of {poet} in Norwegian\"\n",
    "run_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87aaf159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hvordan påvirker diabetes insulinproduksjonen?', 'Hvordan balansere karbohydrater?', 'Hvilke medisiner brukes vanligvis til å behandle type 2 diabetes?', 'Hvordan kan stress påvirke blodsukkernivået?', 'Hvordan kan jeg snakke med familien min om min diabetes?']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Temabasert ordliste\n",
    "temaer = {\n",
    "    \"sykdomsmekanismer\": [\"Hva er diabetes?\", \"Hvordan påvirker diabetes insulinproduksjonen?\"],\n",
    "    \"kostholdsråd\": [\"Hvilke matvarer bør diabetikere unngå?\", \"Hvordan balansere karbohydrater?\"],\n",
    "    \"medisinering\": [\"Hvilke medisiner brukes vanligvis til å behandle type 2 diabetes?\", \"Hvordan fungerer insulinpumper?\"],\n",
    "    \"livsstil\": [\"Hvilke øvelser er trygge for diabetikere?\", \"Hvordan kan stress påvirke blodsukkernivået?\"],\n",
    "    \"psykologisk støtte\": [\"Hvordan kan jeg snakke med familien min om min diabetes?\", \"Hvordan takle depresjon som følge av diabetes?\"]\n",
    "}\n",
    "\n",
    "# Spørsmålsgenerator\n",
    "def generer_spørsmål(tema):\n",
    "    spørsmål = random.choice(temaer[tema])\n",
    "    return spørsmål\n",
    "\n",
    "# Generere et sett med spørsmål\n",
    "def generer_spørsmålsett():\n",
    "    spørsmålsett = []\n",
    "    for tema in temaer.keys():\n",
    "        spørsmålsett.append(generer_spørsmål(tema))\n",
    "    return spørsmålsett\n",
    "\n",
    "# Eksempel på genererte spørsmål\n",
    "print(generer_spørsmålsett())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0bf3ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_liste = generer_spørsmålsett()\n",
    "len(spm_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6e0a020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spm 0: Hva er diabetes?\n",
      "LLM Response:\n",
      "Svar på Spm 0: Diabetes er en kronisk sykdom som oppstår når kroppen har problemer med å regulere blodsukkernivåene. Det finnes hovedsakelig to typer diabetes: type 1, hvor kroppen ikke produserer insulin, og type 2, hvor kroppen ikke bruker insulin effektivt. Insulin er et hormon som er nødvendig for å transportere glukose fra blodet til cellene, der det brukes som energi. Ubehandlet diabetes kan føre til alvorlige helseproblemer, inkludert hjertesykdom, nyreskader og nerveproblemer. Symptomene inkluderer økt tørst, hyppig vannlating, tretthet og vekttap. Behandling kan innebære livsstilsendringer, medisiner og, i noen tilfeller, insulinbehandling. Tidlig diagnose og riktig behandling er avgjørende for å håndtere sykdommen og forebygge komplikasjoner.\n",
      "\n",
      "Spm 1: Hvilke matvarer bør diabetikere unngå?\n",
      "LLM Response:\n",
      "Svar på Spm 1: Diabetikere bør unngå matvarer med høyt innhold av sukker og enkle karbohydrater, som søtsaker, kaker og brus. Hvitt brød og pasta laget av raffinerte korn bør også begrenses, da de kan føre til rask blodsukkerstigning. Fete og bearbeidede matvarer, som fettrike meieriprodukter og hurtigmat, kan påvirke både blodsukkernivået og kolesterolnivået negativt. I tillegg er det lurt å unngå snacks med høyt saltinnhold og transfett. Alkohol kan også være problématisk og bør konsumeres med forsiktighet. Generelt er det viktig å fokusere på en balansert diett med mye fiber, fulle kornprodukter, magre proteiner og sunnere fetter for å opprettholde stabilt blodsukker.\n",
      "\n",
      "Spm 2: Hvilke medisiner brukes vanligvis til å behandle type 2 diabetes?\n",
      "LLM Response:\n",
      "Svar på Spm 2: Vanlige medisiner for å behandle type 2 diabetes inkluderer metformin, som førstelinjebehandling som forbedrer insulinfølsomheten og reduserer glukoseproduksjonen i leveren. Sulfonylurea (som glibenklamid) stimulerer insulinfrigjøring fra bukspyttkjertelen. DPP-4 hemmere (f.eks. sitaglipin) øker insulinproduksjonen og reduserer glukagon. GLP-1 reseptoragonister (som liraglutid) øker insulinproduksjonen i respons på måltider og reduserer appetitt. SGLT2-hemmere (som empagliflozin) reduserer glukoseopptaket i nyrene og fremmer glukoseutskillelse i urinen. Insulin kan også brukes, særlig i avanserte tilfeller. Valg av medisin avhenger av individuelle behov, helsetilstand og legens vurdering.\n",
      "\n",
      "Spm 3: Hvordan kan stress påvirke blodsukkernivået?\n",
      "LLM Response:\n",
      "Svar på Spm 3: Stress kan ha en betydelig effekt på blodsukkernivået. Når kroppen opplever stress, aktiveres stresshormonene, som adrenalin og kortisol. Disse hormonene kan føre til at leveren frigjør mer glukose i blodet for å gi ekstra energi, noe som kan resultere i forhøyet blodsukker. Hos personer med diabetes kan dette være spesielt problematisk, da det kan gjøre blodsukkerkontroll vanskeligere. I tillegg kan stress påvirke matvaner og mosjonsnivå, noe som også kan påvirke blodsukkernivået negativt. Langvarig stress kan dermed bidra til kronisk forhøyede blodsukkernivåer og økt risiko for diabeteskomplikasjoner. Det er derfor viktig å håndtere stress gjennom avslapningsteknikker, mosjon og sunne livsstilsvalg for å opprettholde stabilt blodsukker.\n",
      "\n",
      "Spm 4: Hvordan takle depresjon som følge av diabetes?\n",
      "LLM Response:\n",
      "Svar på Spm 4: Å takle depresjon som følge av diabetes kan være utfordrende, men det er mulig. Først er det viktig å søke profesjonell hjelp, som psykoterapi eller rådgivning, for å få verktøy til å håndtere følelsene. Fysisk aktivitet kan også bidra til bedre stemning og insulinregulering, så prøv å inkludere regelmessig trening i hverdagen. Et sunt kosthold som støtter blodsukkerkontrollen kan også ha positive effekter på humøret. Sørg for å opprettholde sosiale forbindelser; snakk med venner eller familie om hva du gjennomgår. Mindfulness, meditasjon eller yoga kan hjelpe med å redusere stress og angst. Til slutt, vurder å delta i støttegrupper for personer med diabetes, da deling av erfaringer kan være svært helsefremmende.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, spm in enumerate(spm_liste):\n",
    "    print(f'Spm {i}: {spm}')\n",
    "    prompt = f\"{spm}. Svar på norsk. Bruk max 150 ord. Start med 'Svar på Spm {i}'. Ikke bruk avsnitt. -m gpt-4o-mini\"\n",
    "    run_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e973ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.3",
   "language": "sage",
   "name": "sagemath-10.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
