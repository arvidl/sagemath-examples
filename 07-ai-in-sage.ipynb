{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dedd2a6",
   "metadata": {},
   "source": [
    "# 07-ai-in-sage.ipynb\n",
    "\n",
    "\n",
    "A.L. 2023-08-21 (Ubuntu 2022.04, Sage 10.1)\n",
    "\n",
    "- https://github.com/jupyterlab/jupyter-ai\n",
    "\n",
    "- https://openai.com/api/\n",
    "\n",
    "You can find your API key at https://platform.openai.com/account/api-keys.\n",
    "\n",
    "\n",
    "```\n",
    "echo \"export OPENAI_API_KEY='yourkey'\" >> ~/.zshrc   (or  ~/.bashrc )\n",
    "source ~/.zshrc\n",
    "echo $OPENAI_API_KEY\n",
    "```\n",
    "or\n",
    "```\n",
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "\n",
    "\n",
    "See also: https://sagemanifolds.obspm.fr/install_ubuntu.html\n",
    "\n",
    "Check sage installation and start a Jupyter notebook with the SageMath 9.6 kernel:\n",
    "\n",
    "```\n",
    "sage -c 'print(version())\n",
    "sage -notebook\n",
    "```\n",
    "\n",
    "\n",
    "Installing R-packages:\n",
    "```\n",
    "sage -R\n",
    "\n",
    "> install.packages(\"lme4\")\n",
    "```\n",
    "\n",
    "Installing Python packages (in the sage kernel):\n",
    "```\n",
    "> sage --pip install pandas\n",
    "> sage --pip install seaborn\n",
    "> sage --pip install networkx\n",
    "> sage --pip install igraph\n",
    "> # sage --pip install pycairo\n",
    "> # sage --pip install cairocffi\n",
    "> # sage --pip install leidenalg\n",
    "> sage --pip install jupyterlab\n",
    "> sage --pip install altair vega_datasets      # https://altair-viz.github.io/index.html\n",
    "> sage --pip install pygraphviz\n",
    "\n",
    "> sage --pip install openai\n",
    "> sage --pip install jupyter_ai\n",
    "> sage --pip install python-dotenv\n",
    "```\n",
    "\n",
    "```\n",
    "%reload_ext jupyter_ai\n",
    "\n",
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60c30bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211036a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -r, --reset                     Clears the conversation transcript used when\n",
      "                                  interacting with an OpenAI chat model\n",
      "                                  provider. Does nothing with other providers.\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f490af14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:anthropic.claude-v2`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `azure-chat-openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `gpt4all:ggml-gpt4all-j-v1.2-jazzy`, `gpt4all:ggml-gpt4all-j-v1.3-groovy`, `gpt4all:ggml-gpt4all-l13b-snoozy` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-16k`, `openai-chat:gpt-3.5-turbo-0301`, `openai-chat:gpt-3.5-turbo-0613`, `openai-chat:gpt-3.5-turbo-16k-0613`, `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-0613`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-4-32k-0613` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-16k`, `openai-chat-new:gpt-3.5-turbo-0301`, `openai-chat-new:gpt-3.5-turbo-0613`, `openai-chat-new:gpt-3.5-turbo-16k-0613`, `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-0613`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-4-32k-0613` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n"
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x7fa1d7970ca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "201a507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n"
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x7fa1d79a36d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b36356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY=os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e9e5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "OPENAI_API_KEY=$API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f8a18544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\frac{\\partial u}{\\partial t} = \\alpha \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right)\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate the 2D heat equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "958e0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55989eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcm(a, b):\n",
    "    # Find the maximum of a and b\n",
    "    if a > b:\n",
    "        max_num = a\n",
    "    else:\n",
    "        max_num = b\n",
    "\n",
    "    # Iterate and find the lowest common multiple\n",
    "    while True:\n",
    "        if max_num % a == 0 and max_num % b == 0:\n",
    "            return max_num\n",
    "        max_num += 1\n",
    "\n",
    "def run_lcm_tests():\n",
    "    # Run test cases\n",
    "    test_cases = [(3, 5), (9, 12), (8, 14), (15, 20), (24, 36)]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        result = lcm(a, b)\n",
    "        print(f\"LCM of {a} and {b} is {result}\")\n",
    "\n",
    "# Run the test cases\n",
    "run_lcm_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d495c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = \"Herman Wildenvvey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52b6dee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can certainly write a poem in the style of Herman Wildenvey, but I apologize as I can only provide the output in plain text format. Here's the poem:\n",
       "\n",
       "```markdown\n",
       "## Tilflukt\n",
       "\n",
       "Skyene seiler i øde vidder,\n",
       "over fjellene i en vindløs natt.\n",
       "En stjerne faller og dør i flidder,\n",
       "hvorpå mørket tilfredsstilt tar fatt.\n",
       "\n",
       "Havet ruges av evig lengsel,\n",
       "våkenhet flettes i bølgens vev.\n",
       "En ensom måke blir fanget i strengsel,\n",
       "mens horisonten brekker i sitt svev.\n",
       "\n",
       "Fjorden smyger seg dynket i tåke,\n",
       "likt sår hud i vinterens grep.\n",
       "En gråmalt båt driver fritt, å se,\n",
       "hvor endeløst håp og tungsinn svep.\n",
       "\n",
       "Og jeg, en sjømann uten retning,\n",
       "lider i mine drømmers by.\n",
       "Men blant sjøens sang og fjellets lyd,\n",
       "finner jeg tilflukt og evig styring.\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Write a poem in the style of {poet} in Norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ec34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.1",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
