{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dedd2a6",
   "metadata": {},
   "source": [
    "# 07-ai-in-sage.ipynb\n",
    "\n",
    "\n",
    "A.L. 2023-08-21 (Ubuntu 2022.04, Sage 10.1)<br>\n",
    "A.L. 2024-07-22 (MacBook Air 11 inch, Mid 20212, MacOs 10.15.7, Sage 10.3, using `brew install llm`)\n",
    "\n",
    "\n",
    "- https://github.com/jupyterlab/jupyter-ai\n",
    "\n",
    "- https://openai.com/api/\n",
    "\n",
    "You can find your API key at https://platform.openai.com/account/api-keys.\n",
    "\n",
    "\n",
    "```\n",
    "echo \"export OPENAI_API_KEY='yourkey'\" >> ~/.zshrc   (or  ~/.bashrc )\n",
    "source ~/.zshrc\n",
    "echo $OPENAI_API_KEY\n",
    "```\n",
    "or\n",
    "```\n",
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "```\n",
    "\n",
    "\n",
    "See also: https://sagemanifolds.obspm.fr/install_ubuntu.html\n",
    "\n",
    "Check sage installation and start a Jupyter notebook with the SageMath 9.6 kernel:\n",
    "\n",
    "```\n",
    "sage -c 'print(version())\n",
    "sage -notebook\n",
    "```\n",
    "\n",
    "\n",
    "Installing R-packages:\n",
    "```\n",
    "sage -R\n",
    "\n",
    "> install.packages(\"lme4\")\n",
    "```\n",
    "\n",
    "Installing Python packages (in the sage kernel):\n",
    "```\n",
    "> sage --pip install pandas\n",
    "> sage --pip install seaborn\n",
    "> sage --pip install networkx\n",
    "> sage --pip install igraph\n",
    "> # sage --pip install pycairo\n",
    "> # sage --pip install cairocffi\n",
    "> # sage --pip install leidenalg\n",
    "> sage --pip install jupyterlab\n",
    "> sage --pip install altair vega_datasets      # https://altair-viz.github.io/index.html\n",
    "> sage --pip install pygraphviz\n",
    "\n",
    "> sage --pip install openai\n",
    "> sage --pip install jupyter_ai\n",
    "> sage --pip install python-dotenv\n",
    "```\n",
    "\n",
    "```\n",
    "%reload_ext jupyter_ai\n",
    "\n",
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60c30bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "211036a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "  version   Prints Jupyter-AI version\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f490af14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li></ul> |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x16956ce90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "201a507b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is no model provider with ID `openai`."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x16951b250>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b36356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY=os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e9e5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "OPENAI_API_KEY=$API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8a18544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x16954db10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate the 2D heat equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "958e0b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x168effcd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt --format code\n",
    "A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55989eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCM of 3 and 5 is 15\n",
      "LCM of 9 and 12 is 36\n",
      "LCM of 8 and 14 is 56\n",
      "LCM of 15 and 20 is 60\n",
      "LCM of 24 and 36 is 72\n",
      "LCM of 9 and 28 is 252\n"
     ]
    }
   ],
   "source": [
    "def lcm(a, b):\n",
    "    # Find the maximum of a and b\n",
    "    if a > b:\n",
    "        max_num = a\n",
    "    else:\n",
    "        max_num = b\n",
    "\n",
    "    # Iterate and find the lowest common multiple\n",
    "    while True:\n",
    "        if max_num % a == 0 and max_num % b == 0:\n",
    "            return max_num\n",
    "        max_num += 1\n",
    "\n",
    "def run_lcm_tests():\n",
    "    # Run test cases\n",
    "    test_cases = [(3, 5), (9, 12), (8, 14), (15, 20), (24, 36), (9,28)]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        result = lcm(a, b)\n",
    "        print(f\"LCM of {a} and {b} is {result}\")\n",
    "\n",
    "# Run the test cases\n",
    "run_lcm_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d495c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "poet = \"Herman Wildenvvey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52b6dee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Cannot determine model provider from model ID `chatgpt`.\n",
       "\n",
       "To see a list of models you can use, run `%ai list`\n",
       "\n",
       "If you were trying to run a command, run `%ai help` to see a list of commands."
      ],
      "text/plain": [
       "<jupyter_ai_magics.magics.TextOrMarkdown object at 0x168f0e4d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt \n",
    "Write a poem in the style of {poet} in Norwegian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247ddeb",
   "metadata": {},
   "source": [
    "## llm   Simon Willison\n",
    "\n",
    "https://github.com/simonw/llm\n",
    "\n",
    "https://llm.datasette.io/en/stable/usage.html\n",
    "\n",
    "```bash\n",
    "If you have an OpenAI API key you can get started using the OpenAI models right away.\n",
    "\n",
    "As an alternative to OpenAI, you can install plugins to access models by other providers, including models that can be installed and run on your own device.\n",
    "\n",
    "Save your OpenAI API key like this:\n",
    "\n",
    "llm keys set openai\n",
    "This will prompt you for your key like so:\n",
    "\n",
    "Enter key: <paste here>\n",
    "```\n",
    "\n",
    "Now that you've saved a key you can run a prompt like this:\n",
    "```bash\n",
    "llm \"Five cute names for a pet penguin\"\n",
    "```\n",
    "\n",
    "The OpenAI API key is saved in keys.json:\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % ls -l /Users/arvid/Library/Application\\ Support/io.datasette.llm\n",
    "total 136\n",
    "-rw-------  1 arvid  staff    146 Jul 20 17:47 keys.json\n",
    "-rw-r--r--  1 arvid  staff  65536 Jul 20 20:09 logs.db\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm models\n",
    "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
    "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
    "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
    "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
    "OpenAI Chat: gpt-4-1106-preview\n",
    "OpenAI Chat: gpt-4-0125-preview\n",
    "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
    "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
    "OpenAI Chat: gpt-4o (aliases: 4o)\n",
    "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
    "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
    "```\n",
    "\n",
    "To describe how the code a file works, try this:\n",
    "```bash\n",
    "cat mycode.py | llm -s \"Explain this code\"\n",
    "```\n",
    "\n",
    "Additional examples and functions:\n",
    "\n",
    "```bash\n",
    "llm 'Ten names for cheesecakes' -m gpt-4o\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm models --options\n",
    "OpenAI Chat: gpt-3.5-turbo (aliases: 3.5, chatgpt)\n",
    "  temperature: float\n",
    "    What sampling temperature to use, between 0 and 2. Higher values like\n",
    "    0.8 will make the output more random, while lower values like 0.2 will\n",
    "    make it more focused and deterministic.\n",
    "  max_tokens: int\n",
    "    Maximum number of tokens to generate.\n",
    "  top_p: float\n",
    "    An alternative to sampling with temperature, called nucleus sampling,\n",
    "    where the model considers the results of the tokens with top_p\n",
    "    probability mass. So 0.1 means only the tokens comprising the top 10%\n",
    "    probability mass are considered. Recommended to use top_p or\n",
    "    temperature but not both.\n",
    "  frequency_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on their existing frequency in the text so far, decreasing the model's\n",
    "    likelihood to repeat the same line verbatim.\n",
    "  presence_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on whether they appear in the text so far, increasing the model's\n",
    "    likelihood to talk about new topics.\n",
    "  stop: str\n",
    "    A string where the API will stop generating further tokens.\n",
    "  logit_bias: dict, str\n",
    "    Modify the likelihood of specified tokens appearing in the completion.\n",
    "    Pass a JSON string like '{\"1712\":-100, \"892\":-100, \"1489\":-100}'\n",
    "  seed: int\n",
    "    Integer seed to attempt to sample deterministically\n",
    "  json_object: boolean\n",
    "    Output a valid JSON object {...}. Prompt must mention JSON.\n",
    "OpenAI Chat: gpt-3.5-turbo-16k (aliases: chatgpt-16k, 3.5-16k)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4 (aliases: 4, gpt4)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-32k (aliases: 4-32k)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-1106-preview\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-0125-preview\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-turbo-2024-04-09\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4-turbo (aliases: gpt-4-turbo-preview, 4-turbo, 4t)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4o (aliases: 4o)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n",
    "  temperature: float\n",
    "  max_tokens: int\n",
    "  top_p: float\n",
    "  frequency_penalty: float\n",
    "  presence_penalty: float\n",
    "  stop: str\n",
    "  logit_bias: dict, str\n",
    "  seed: int\n",
    "  json_object: boolean\n",
    "OpenAI Completion: gpt-3.5-turbo-instruct (aliases: 3.5-instruct, chatgpt-instruct)\n",
    "  temperature: float\n",
    "    What sampling temperature to use, between 0 and 2. Higher values like\n",
    "    0.8 will make the output more random, while lower values like 0.2 will\n",
    "    make it more focused and deterministic.\n",
    "  max_tokens: int\n",
    "    Maximum number of tokens to generate.\n",
    "  top_p: float\n",
    "    An alternative to sampling with temperature, called nucleus sampling,\n",
    "    where the model considers the results of the tokens with top_p\n",
    "    probability mass. So 0.1 means only the tokens comprising the top 10%\n",
    "    probability mass are considered. Recommended to use top_p or\n",
    "    temperature but not both.\n",
    "  frequency_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on their existing frequency in the text so far, decreasing the model's\n",
    "    likelihood to repeat the same line verbatim.\n",
    "  presence_penalty: float\n",
    "    Number between -2.0 and 2.0. Positive values penalize new tokens based\n",
    "    on whether they appear in the text so far, increasing the model's\n",
    "    likelihood to talk about new topics.\n",
    "  stop: str\n",
    "    A string where the API will stop generating further tokens.\n",
    "  logit_bias: dict, str\n",
    "    Modify the likelihood of specified tokens appearing in the completion.\n",
    "    Pass a JSON string like '{\"1712\":-100, \"892\":-100, \"1489\":-100}'\n",
    "  seed: int\n",
    "    Integer seed to attempt to sample deterministically\n",
    "  logprobs: int\n",
    "    Include the log probabilities of most likely N per token\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```\n",
    "\n",
    "OpenAI embedding models\n",
    "```bash\n",
    "(base) arvid@Arvids-Air ~ % llm embed-models\n",
    "ada-002 (aliases: ada)\n",
    "3-small\n",
    "3-large\n",
    "3-small-512\n",
    "3-large-256\n",
    "3-large-1024\n",
    "(base) arvid@Arvids-Air ~ % \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b41ae6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_llm(prompt):\n",
    "    binary_path = \"/usr/local/bin/llm\"\n",
    "    command = [binary_path, prompt]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, text=True, capture_output=True)\n",
    "        \n",
    "        print(\"LLM Response:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"Standard Error:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        print(f\"Return Code: {result.returncode}\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(f\"Standard Output: {e.stdout}\")\n",
    "        print(f\"Standard Error: {e.stderr}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd9fa8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "Certainly! To compute the Lowest Common Multiple (LCM) of two integers, we can use the relationship between LCM and the Greatest Common Divisor (GCD). The formula is:\n",
      "\n",
      "\\[\n",
      "\\text{LCM}(a, b) = \\frac{|a \\times b|}{\\text{GCD}(a, b)}\n",
      "\\]\n",
      "\n",
      "Here's how you can implement this in Python, along with a function that tests five different cases:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "# Function to compute the LCM of two integers\n",
      "def lcm(a, b):\n",
      "    if a == 0 or b == 0:\n",
      "        return 0  # LCM is undefined for 0\n",
      "    return abs(a * b) // math.gcd(a, b)\n",
      "\n",
      "# Function to run test cases for the LCM function\n",
      "def test_lcm():\n",
      "    test_cases = [\n",
      "        (12, 15),   # LCM should be 60\n",
      "        (7, 5),     # LCM should be 35\n",
      "        (0, 10),    # LCM should be 0\n",
      "        (9, 3),     # LCM should be 9\n",
      "        (21, 6),    # LCM should be 42\n",
      "    ]\n",
      "    \n",
      "    for a, b in test_cases:\n",
      "        result = lcm(a, b)\n",
      "        print(f\"LCM({a}, {b}) = {result}\")\n",
      "\n",
      "# Run the test cases\n",
      "test_lcm()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **LCM Function**: \n",
      "   - The `lcm` function takes two integer arguments, `a` and `b`.\n",
      "   - It returns 0 if either `a` or `b` is 0 since the LCM is undefined in such cases.\n",
      "   - Otherwise, it computes and returns the LCM using the absolute value of the product divided by the GCD.\n",
      "\n",
      "2. **Test Cases**:\n",
      "   - The `test_lcm` function defines a list of test case tuples. Each tuple contains two integers for which the LCM needs to be calculated.\n",
      "   - It uses a loop to call the `lcm` function for each pair and prints the result.\n",
      "\n",
      "You can run this code in a Python environment to check the lowest common multiples of the specified test cases!\n",
      "\n",
      "Return Code: 0\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A function that computes the lowest common multiples of two integers, and a function that runs 5 test cases of the lowest common multiple function\"\n",
    "run_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a3cde65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcm(15, 20) = 60\n",
      "lcm(9, 28) = 252\n",
      "lcm(5, 0) = 0\n",
      "lcm(0, 10) = 0\n",
      "lcm(12, 15) = 60\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def lcm(a, b):\n",
    "    \"\"\"Calculate the least common multiple of two integers a and b.\"\"\"\n",
    "    if a == 0 or b == 0:\n",
    "        return 0  # LCM of zero with any number is defined as zero\n",
    "    \n",
    "    return abs(a * b) // math.gcd(a, b)\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"Run a set of test cases for the lcm function.\"\"\"\n",
    "    test_cases = [\n",
    "        (15, 20),  # Expected LCM: 60\n",
    "        (9, 28),   # Expected LCM: 252\n",
    "        (5, 0),    # Expected LCM: 0 (with zero)\n",
    "        (0, 10),   # Expected LCM: 0 (with zero)\n",
    "        (12, 15)   # Expected LCM: 60\n",
    "    ]\n",
    "    \n",
    "    for a, b in test_cases:\n",
    "        print(f\"lcm({a}, {b}) = {lcm(a, b)}\")\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6ed2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "I Herman Wildenveys stil, her er et dikt på norsk:\n",
      "\n",
      "**Måneskinn som hvisker**\n",
      "\n",
      "Under stjernespeinet natten,\n",
      "der drømmer danser, lett som luft,\n",
      "blomsterbedd i dypet av sjelen,\n",
      "hvor minner vokser, stille, skjult.\n",
      "\n",
      "En elv av skygger, sølv som glitrer,\n",
      "strømmer gjennom tidens sår,\n",
      "med minner som fugler, lett de svinner,\n",
      "bærer hjertet mot hvor kjærlighet står.\n",
      "\n",
      "Måneskinn som hvisker, mildt og stille,\n",
      "forteller om kvelder av gull,\n",
      "om kyss som brenner, hjerter som rille,\n",
      "i lagen av tid, i bølgens kul.\n",
      "\n",
      "Gi meg et glimt av det som var,\n",
      "en dans i de dypeste drømmer,\n",
      "hvor hjerter slår, hvor sjelen svarer,\n",
      "og kjærlighet vokser av sorgens klemmer.\n",
      "\n",
      "Så la meg vandre i månens skinn,\n",
      "gennom skogen av minner og lys,\n",
      "hvor hver hvisking er et annet sinn,\n",
      "og hver sti er et livs bevis.\n",
      "\n",
      "I denne natten, i disse vers,\n",
      "går vi mot horisonter, usette, frie,\n",
      "for kjærlighetens kraft er alltid nær,\n",
      "i måneskinnets milde melodi.\n",
      "\n",
      "Return Code: 0\n"
     ]
    }
   ],
   "source": [
    "poet = \"Herman Wildenvvey\"\n",
    "prompt = f\"Write a poem in the style of {poet} in Norwegian\"\n",
    "run_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aaf159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.3",
   "language": "sage",
   "name": "sagemath-10.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
